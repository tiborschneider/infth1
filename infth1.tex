%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass[a4paper]{article}

\input{fs.tex}
%Title of Document
\chead{Information Theory I \@ Summary} 

\begin{document}
\begin{twocolumn} 

\section{Definitions}

\begin{itemize}
    \item \textbf{Chance Variable}: Outcome of random Experiment, where the outcome does not need to be a number
    \item \textbf{Random Variable}: ($X \in \mathbb{R}$) is random wit a certan probability distribution. From a random variable, one can compute $X^2$, $\E[X], \var(X)$.
    \item \textbf{Probability Mass Funtion (PMF)}: $\Pr[X=x] = \mathcal{P}(x) = \mathcal{P}_X(x)$
    \item \textbf{Alphabet}: ($X \in \mathcal{X}$), where $\mathcal{X}$ is the alphabet
\end{itemize}

\subsection{Uncertanty (Entropy)}

Uncertanty of a random variable is defined as:
\[H(X) = \sum_{x \in \mathcal{X}} \mathcal{P}(x) \log \frac{1}{\mathcal{P}(x)} = E_X \left[ \log \frac{1}{\mathcal{P}(x)} \right] \qquad 0 \leq H(X) \leq \log \abs{\mathcal{X}}\]

\begin{itemize}
    \item Lower bound: $H(X) = 0$ if and only if $\exists x \in \mathcal{X}$ s.t. $\mathcal{P}(x) = 1$
    \item Upper bound: $H(X) = \log \abs{\mathcal{X}}$ if and only if $\mathcal{P}(x) = \frac{1}{\abs{\mathcal{X}}} \forall x \in \mathcal{X}$
\end{itemize}

Note that the basis of the logarithm defines the unit. For base 2: the unit is called a \textbf{bit}, and for the natural logarithm, the unit is called \textbf{nat} (for natural).

$H_b$ denotes binary uncertanty. Let $X \in \{0,1\}$ and $\matcal{P}(X=1) = 1-\mathcal{P}(X=0) = p$. Then:
\[H_b = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}\]

\subsubsection{Relative Entropy}

We want to compare two chance variables the same alphabet $\mathcal{X}$ with different PMF $p(x)$ and $q(x)$. The relative entropy is defined as:
\[D\left(p(\cdot) || q(\cdot) \right) \triangleq \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}\]
\[D(p||q) \geq 0 \quad \text{and} \quad D(p||q) = 0 \text{ if and only if } p = q\]
We have the inequality, that $H(X,Y) \leq H(X) + H(Y)$ with equality $H(X,Y) = H(X) + H(Y)$ if and only if $X$ and $Y$ are independent.

\subsubsection{Joint Uncertanty}
We have two chance variables $X \in \mathcal{X}$ and $Y \in \mathcal{Y}$ with PMF $\mathcal{P}(x,y)$. The Joint Uncertanty is:
\[H(X,Y) = \sum_{x \in \mathcal{X},\ y \in \mathcal{Y}} \mathcal{P}(x,y) \log \frac{1}{\mathcal{P}(x,y)} = E_{X,Y} \left[ -\log \mathcal{P}(x,y) \right]\]

The \textbf{chain rule} for Joint Entropy is:
\[H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)\]
\[H(X_1,X_2,\ldots,X_n) = H(X_1) + H(X_2|X_1) + \cdots + H(X_n|X_1,\ldots,X_{n-1})\]

We introduce the \textbf{Notation}: $X_k^n \mapsto X_k,\ldots,X_n$ and $X^n \mapsto X_1^n$. Then, the chain rule can be written as:
\[H(X_1^n) = H(X^n) = \sum_i^n H(X_i|X_1^{i-1}) = \sum_i^n H(X_i|X_{i+1}^n)\]


\subsubsection{Conditional Entropy}
The conditional entropy given the Event $Y = y \in \mathcal{Y}$ 
\[H(X|Y=y) = \sum_{x \in \mathcal{X}} \mathcal{P}(x|Y=y) \log \frac{1}{\mathcal{P}(x|Y=y)}\]
\[H(X|Y) = \sum_{y \in \mathcal{Y}} \mathcal{P}(y) H(X|Y=y) = \sum_{x \in \mathcal{X},\ y \in \mathcal{Y}} \mathcal{P}(x,y) \log \frac{1}{\mathcal{P}(x|y)}\]
With the information from $Y$, we know more about $X$: $H(X) \geq H(X|Y)$

\subsubsection{Fano's Inequality}
Consider $X \in \mathcal{X}$ and $Y \in \mathcal{Y}$ with a joint PMF $\mathcal{P}(X,Y)$. 
We want to guess $X$ from $Y$ with a function $g(Y): \mathcal{Y} \mapsto \mathcal{X}$
We define $\mathcal{P}_e = \mathcal{P}\left[ g(Y) \neq X \right]$.
No matter how $g(\cdot)$ is defined, the following inequality holds:
\[ 1 + \mathcal{P}_e \log \abs{\mathcal{X}} \geq H_b(\mathcal{P}_e) + \mathcal{P}_e \log \left( \abs{\mathcal{X}-1} \right) \geq H(X|Y) \]

\subsection{Mutual Information}
Mutual Information is the amount of entropy that goes down as soon as you know $Y$:
\[I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\]

\begin{itemize}
    \item $I(X;Y) \geq 0$ with equality $I(X;Y) = 0$ if and only if $X$ and $Y$ are independent
    \item $I(X;Y) = I(Y;X)$
    \item $I(X;Y) = H(X) + H(Y) - H(X|Y)$
    \item $I(X;Y) = D(\mathcal{P}_{XY}||\mathcal{P}_X \cdot \mathcal{P}_Y)$
    \item $I(X;Y|Z) = \sum_{z \in \mathcal{Z}} \mathcal{P}_Z(z) \cdot I(X;Y|Z=z)$
\end{itemize}

\section{Data Crompression}

\subsection{Definitions}

\begin{itemize}
    \item The \textbf{code} $\mathcal{C}(X)$ is a Mapping $\mathcal{C}:\ \mathcal{X} \mapsto {\{0,1\}}^+$.
        The Output ${\{0,1\}}^+$ is a string of finite nonzero length.
    \item A \textbf{one-to-variable} code $\mathcal{C}$ which maps one random variable to a striing of variable length. 
        In comparison, a \textbf{one-to-fixed} code $\mathcal{C}$ maps the input to a string of fixed length.
    \item The code $\mathcal{C}$ is \textbf{nonsingular} if $\mathcal{C}(x) = \mathcal{C}(x') \mapsto x = x'$
    \item The \textbf{extension} $\mathcal{C}^+$ of a code $\mathcal{C}$ is a mapping $\mathcal{C}^+: \mathcal{X}^+ \mapsto {\{ 0,1 \}}^+$ and is defined as:
        \[ \mathcal{C}^+(x_1, \ldots, x_n) = \mathcal{C}(x_1) \mathcal{C}(x_2) \ldots \mathcal{C}(x_n) \]
    \item The code $\mathcal{C}$ is \textbf{uniquely decodable} (u.d.) if $\mathcal{C}^+$ is nonsingular. 
        (Meaning there cannot be two input strings which produce the same code $\mathcal{C}^+$)
    \item The code $\mathcal{C}$ is \textbf{prefix free} if no codeword is a prefix of another. 
        Every prefix free code is also uniquely decodable.
    \item The \textbf{expected description length} $\mathcal{L}$ is defined as follows, where $\ell(\mathcal{C}(x))$ is the length of the encoded string, and $\mathcal{L}^\ast$ is smallest length possible.
        \[ \mathcal{L} = \sum_{x \in \mathcal{X}} \mathcal{P}_X(x) \cdot \ell(\mathcal{C}(x))\qquad \mathcal{L}^\ast = \min_{\mathcal{C}} \mathcal{L}(\mathcal{C}) \] %TODO
\end{itemize}

\subsection{Kraft's Inequality}

Let $x \in \mathcal{X}$ and let $\ell_1, \ldots, \ell_{\abs{\mathcal{X}}} > 0$ be positive integers. 
Then, there exists a prefix-free (and therefore a uniquely decodable) Code $\mathcal{C}$ with lengths $\ell_1, \ldots, \ell_\abs{\mathcal{X}}$, if those lengths satisfy the following condition:

\[ \sum_{k = 1}^{\abs{\mathcal{X}}} 2^{-\ell_k} \leq 1 \]




\section{General Formulas}
\subsection{Inequalities}

\begin{mtabular}{lll}
    \textbf{Name} & \textbf{condition} &\textbf{Inequality} \\ \toprule
    Uncertanty & $X \in \mathcal{X}$ & $\fmm 0 \leq H(X) \leq \log \abs{\mathcal{X}}$ \\
               & \multicolumn{2}{l}{$H(X) = 0 \iff \exists x \in \mathcal{X}$, s.t. $\mathcal{P}(x) = 1$} \\
               & \multicolumn{2}{l}{$\fmm H(X) = \log \abs{\mathcal{X}} \iff \mathcal{P}(x) = \log \frac{1}{\abs{\mathcal{X}}} \forall x \in \mathcal{X}$} \\
    Rel. Entropy & $X \in \mathcal{X}, \ X \in \mathcal{Y}$ & $H(X,Y) \leq H(X) + H(Y)$ \\
                 & \multicolumn{2}{l}{$H(X,Y) = H(X) + H(Y) \iff X \text{ and } Y \text{ are independent.}$}\\
    Cond. Entropy & $X \in \mathcal{X}$ & $H(X) \geq H(X|Y)$ \\
    
    IT ineq. & Logarithm & $\fmm \left(1-\frac{1}{\xi}\right) \log_b(e) \leq \log_b(\xi) \leq (\xi-1) \log_b(e)$ \\
    Jensen ineq. & $f$ concave in $[a,b]$ & $\E\left[ f(x) \right] \leq f\left(  \right)$ \\
Kraft's ineq. & $\mathcal{C}$ is u.d. & $\sum_{x \in \mathcal{X}} 2^{-\ell(\mathcal{C}(x))} \leq 1$ 

\end{mtabular}


\end{twocolumn}
\end{document}
